{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de010c4bc20e4191a2dafd5a473965a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_138a2b9429ed42529b51a075c5418dd8",
              "IPY_MODEL_0b6e2a7d9c6a4d6b9f899bfced7a6953",
              "IPY_MODEL_6275d2022ac948be95f05b7f9343126a"
            ],
            "layout": "IPY_MODEL_83a4f8f457104e5281646b023e9183fa"
          }
        },
        "138a2b9429ed42529b51a075c5418dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae78351e5aa24c6cac3484d6210489f7",
            "placeholder": "​",
            "style": "IPY_MODEL_f84830e0648b4493bb646250d816767a",
            "value": "config.json: 100%"
          }
        },
        "0b6e2a7d9c6a4d6b9f899bfced7a6953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18bb4f63b6044dafa202d36c5a7bf5b8",
            "max": 69556,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4d3f1acad32491abff3f8b01822ccfa",
            "value": 69556
          }
        },
        "6275d2022ac948be95f05b7f9343126a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed4aa0e051b4816a459e99f2c7e95e7",
            "placeholder": "​",
            "style": "IPY_MODEL_11dc51661ae64b2a95600e4375c3b561",
            "value": " 69.6k/69.6k [00:00&lt;00:00, 1.02MB/s]"
          }
        },
        "83a4f8f457104e5281646b023e9183fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae78351e5aa24c6cac3484d6210489f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f84830e0648b4493bb646250d816767a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18bb4f63b6044dafa202d36c5a7bf5b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d3f1acad32491abff3f8b01822ccfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bed4aa0e051b4816a459e99f2c7e95e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11dc51661ae64b2a95600e4375c3b561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00929cb1fac34342aedd4dc1dacedfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2551063385b54830a8750b8c60d54eda",
              "IPY_MODEL_18ec5db447b04119b1c885deda024fc4",
              "IPY_MODEL_13dd5ee9b9714d45a1c91705312e723a"
            ],
            "layout": "IPY_MODEL_f3527128fe564587ba4509b286a9f72c"
          }
        },
        "2551063385b54830a8750b8c60d54eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb5c864186e143f693f7bd69d6806fe3",
            "placeholder": "​",
            "style": "IPY_MODEL_29fd81643c5c4d5bbb1cedbfb80ce206",
            "value": "model.safetensors: 100%"
          }
        },
        "18ec5db447b04119b1c885deda024fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef15dbdba2d14c2a87a8f010b3881a0f",
            "max": 102482854,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b047f0554d7470b853c85a8c37a78ac",
            "value": 102482854
          }
        },
        "13dd5ee9b9714d45a1c91705312e723a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ea143c9ea6f4ae6ac4bb09822ceaefb",
            "placeholder": "​",
            "style": "IPY_MODEL_a7236f1dee9e4f6faa95166fe51b01be",
            "value": " 102M/102M [00:00&lt;00:00, 205MB/s]"
          }
        },
        "f3527128fe564587ba4509b286a9f72c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb5c864186e143f693f7bd69d6806fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29fd81643c5c4d5bbb1cedbfb80ce206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef15dbdba2d14c2a87a8f010b3881a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b047f0554d7470b853c85a8c37a78ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ea143c9ea6f4ae6ac4bb09822ceaefb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7236f1dee9e4f6faa95166fe51b01be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and device setup"
      ],
      "metadata": {
        "id": "tTrO-X0DCyEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKbaIXZAfkSi",
        "outputId": "7b791205-75d7-423f-acf3-d44e3d0aa40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.5.1+cu121\n",
            "Torchvision version: 0.20.1+cu121\n",
            "Device we use: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Check PyTorch and torchvision versions\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "\n",
        "# Set device to GPU if available else go with cpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device we use:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unzip Datasets(in G.Drive): image_train, image_test"
      ],
      "metadata": {
        "id": "p0SgqcVsC7AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# UNZIPPING THE DATASETS\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/finetuning/images_training_rev1.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/image_train')\n",
        "\n",
        "# Path to the main zip file\n",
        "zip_file_path = '/content/drive/MyDrive/finetuning/images_test_rev1.zip'\n",
        "output_dir = '/content/image_test'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Function to recursively unzip files\n",
        "def unzip_all(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "        for root, _, files in os.walk(extract_to):\n",
        "            for file in files:\n",
        "                if file.endswith('.zip'):\n",
        "                    nested_zip_path = os.path.join(root, file)\n",
        "                    nested_output_dir = os.path.join(root, file.replace('.zip', ''))\n",
        "                    os.makedirs(nested_output_dir, exist_ok=True)\n",
        "                    unzip_all(nested_zip_path, nested_output_dir)  # Recursively unzip nested zip files\n",
        "                    os.remove(nested_zip_path)  # Optionally remove nested zip file after extraction\n",
        "\n",
        "# Unzip the main file and handle nested zips\n",
        "unzip_all(zip_file_path, output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kscm10uSg2oa",
        "outputId": "7366e691-d185-41a7-b0cc-dd3daa00acb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Organize unzipped files"
      ],
      "metadata": {
        "id": "QxVHlYVTDUHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING IMAGE PATH IMAGES AND LABELS\n",
        "image_dir = '/content/image_train/images_training_rev1'  # Path to training images\n",
        "labels_path = '/content/training_solutions_rev1.csv'  # Path to labels CSV\n",
        "\n",
        "# Load labels\n",
        "labels_df = pd.read_csv(labels_path)\n",
        "print(\"Labels loaded:\", labels_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A7hzjRMDIpF",
        "outputId": "eec9b302-9b55-424a-d6f2-9a984624bcbc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels loaded: (61578, 38)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.PreProcessing Data"
      ],
      "metadata": {
        "id": "a2cnSMdFHzz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA AUGUMENTATIONA AND NORMALIZATION FOR TRAINING\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),    # Resize to 100x100 if needed\n",
        "    transforms.ToTensor(),            # Convert to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize\n",
        "])\n"
      ],
      "metadata": {
        "id": "E8wTokvNH7am"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#CUSTOM DTASET CLASS(GALAXY DATASET) FOR LOADING IMAGES AND LABELS AS TENSORS\n",
        "class GalaxyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, labels_df, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.labels_df = labels_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Assuming the image filename is in the first column\n",
        "        img_filename = str(self.labels_df.iloc[idx, 0]) + '.jpg'  # Add .jpg extension if needed\n",
        "        img_name = os.path.join(self.image_dir, img_filename)  # Adjust if there's a different file extension\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_name).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Image file not found: {img_name}\")\n",
        "            raise\n",
        "\n",
        "        label = torch.tensor(self.labels_df.iloc[idx, 1:].values, dtype=torch.float32)  # labels as tensor\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n"
      ],
      "metadata": {
        "id": "xRteKA_3hcqX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting the Dataset"
      ],
      "metadata": {
        "id": "REAyfZe3KGke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and SPLIT TO TRAIN AND VALIDATION SETS AND TEST DATA SETS\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Define the dataset\n",
        "dataset = GalaxyDataset(image_dir, labels_df, transform=transform)\n",
        "\n",
        "batch_size = 37  # or any other number you'd like to use for the batch size\n",
        "\n",
        "\n",
        "# Define sizes for train, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))  # 70% for training\n",
        "valid_size = int(0.15 * len(dataset)) # 15% for validation\n",
        "test_size = len(dataset) - train_size - valid_size  # Remaining 15% for testing\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "# Create data loaders for each split\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "validloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "testloader = DataLoader(test_dataset, batch_size=37, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "Y7yizFKmhfV3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of a batch from the train loader\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)  # Use next(dataiter) instead of dataiter.next()\n",
        "print(\"Train batch shape:\", images.shape)\n",
        "print(\"Train labels batch shape:\", labels.shape)\n",
        "\n",
        "# Check the shape of a batch from the validation loader\n",
        "dataiter_valid = iter(validloader)\n",
        "images_valid, labels_valid = next(dataiter_valid)\n",
        "print(\"Validation batch shape:\", images_valid.shape)\n",
        "print(\"Validation labels batch shape:\", labels_valid.shape)\n",
        "\n",
        "# Check the shape of a batch from the test loader\n",
        "dataiter_test = iter(testloader)\n",
        "images_test, labels_test = next(dataiter_test)\n",
        "print(\"Test batch shape:\", images_test.shape)\n",
        "print(\"Test labels batch shape:\", labels_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "9kufpcxMJC1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b06e1d-a0cd-4f46-b327-a16a88b28193"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batch shape: torch.Size([37, 3, 100, 100])\n",
            "Train labels batch shape: torch.Size([37, 37])\n",
            "Validation batch shape: torch.Size([37, 3, 100, 100])\n",
            "Validation labels batch shape: torch.Size([37, 37])\n",
            "Test batch shape: torch.Size([37, 3, 100, 100])\n",
            "Test labels batch shape: torch.Size([37, 37])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Load Pre-Trained ResNet-50"
      ],
      "metadata": {
        "id": "VzYu_2ISD0c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "# Apply LoRA to the model's last layer\n",
        "\n",
        "\n",
        "model_checkpoint = \"microsoft/resnet-50\"\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=37,\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ],
      "metadata": {
        "id": "-515GvIMDy_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255,
          "referenced_widgets": [
            "de010c4bc20e4191a2dafd5a473965a0",
            "138a2b9429ed42529b51a075c5418dd8",
            "0b6e2a7d9c6a4d6b9f899bfced7a6953",
            "6275d2022ac948be95f05b7f9343126a",
            "83a4f8f457104e5281646b023e9183fa",
            "ae78351e5aa24c6cac3484d6210489f7",
            "f84830e0648b4493bb646250d816767a",
            "18bb4f63b6044dafa202d36c5a7bf5b8",
            "e4d3f1acad32491abff3f8b01822ccfa",
            "bed4aa0e051b4816a459e99f2c7e95e7",
            "11dc51661ae64b2a95600e4375c3b561",
            "00929cb1fac34342aedd4dc1dacedfbf",
            "2551063385b54830a8750b8c60d54eda",
            "18ec5db447b04119b1c885deda024fc4",
            "13dd5ee9b9714d45a1c91705312e723a",
            "f3527128fe564587ba4509b286a9f72c",
            "cb5c864186e143f693f7bd69d6806fe3",
            "29fd81643c5c4d5bbb1cedbfb80ce206",
            "ef15dbdba2d14c2a87a8f010b3881a0f",
            "5b047f0554d7470b853c85a8c37a78ac",
            "9ea143c9ea6f4ae6ac4bb09822ceaefb",
            "a7236f1dee9e4f6faa95166fe51b01be"
          ]
        },
        "outputId": "893a7b62-e016-4aac-86af-cfdec8809364"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de010c4bc20e4191a2dafd5a473965a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00929cb1fac34342aedd4dc1dacedfbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
            "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([37]) in the model instantiated\n",
            "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([37, 2048]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA  Implementation"
      ],
      "metadata": {
        "id": "q0UHHcYEDhJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import AutoModelForImageClassification, AutoFeatureExtractor\n",
        "\n",
        "\n",
        "# Define the LoRA adaptation layer\n",
        "class LORALayer(nn.Module):\n",
        "    def __init__(self, adapted_layer, rank=16):\n",
        "        super(LORALayer, self).__init__()\n",
        "        self.adapted_layer = adapted_layer\n",
        "        self.rank = rank\n",
        "        # Initialize low-rank matrices A and B\n",
        "        self.A = nn.Parameter(torch.randn(adapted_layer.weight.size(1), rank) * 0.01)\n",
        "        self.B = nn.Parameter(torch.randn(rank, adapted_layer.weight.size(0)) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate the low-rank adaptation\n",
        "        low_rank_matrix = self.A @ self.B\n",
        "        adapted_weight = self.adapted_layer.weight + low_rank_matrix.t()\n",
        "        return nn.functional.linear(x, adapted_weight, self.adapted_layer.bias)\n",
        "\n",
        "# Replace the final classification layer with a LoRA layer\n",
        "last_layer = model.classifier[-1]  # Assuming the last layer is linear\n",
        "model.classifier[-1] = LORALayer(last_layer)\n"
      ],
      "metadata": {
        "id": "HAuFKBX-f0fw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "aEjXqMtcj5r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67cd5831-1f78-45d1-858f-bd3f7047c61a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNetForImageClassification(\n",
              "  (resnet): ResNetModel(\n",
              "    (embedder): ResNetEmbeddings(\n",
              "      (embedder): ResNetConvLayer(\n",
              "        (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "        (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "      (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoder): ResNetEncoder(\n",
              "      (stages): ModuleList(\n",
              "        (0): ResNetStage(\n",
              "          (layers): Sequential(\n",
              "            (0): ResNetBottleNeckLayer(\n",
              "              (shortcut): ResNetShortCut(\n",
              "                (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (1): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (2): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): ResNetStage(\n",
              "          (layers): Sequential(\n",
              "            (0): ResNetBottleNeckLayer(\n",
              "              (shortcut): ResNetShortCut(\n",
              "                (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (1): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (2): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (3): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): ResNetStage(\n",
              "          (layers): Sequential(\n",
              "            (0): ResNetBottleNeckLayer(\n",
              "              (shortcut): ResNetShortCut(\n",
              "                (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (1): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (2): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (3): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (4): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (5): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ResNetStage(\n",
              "          (layers): Sequential(\n",
              "            (0): ResNetBottleNeckLayer(\n",
              "              (shortcut): ResNetShortCut(\n",
              "                (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "                (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (1): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "            (2): ResNetBottleNeckLayer(\n",
              "              (shortcut): Identity()\n",
              "              (layer): Sequential(\n",
              "                (0): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (1): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): ReLU()\n",
              "                )\n",
              "                (2): ResNetConvLayer(\n",
              "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "                  (activation): Identity()\n",
              "                )\n",
              "              )\n",
              "              (activation): ReLU()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): LORALayer(\n",
              "      (adapted_layer): Linear(in_features=2048, out_features=37, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Training Loop with LoRA"
      ],
      "metadata": {
        "id": "imH8ivUNB94Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer, criterion, and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "best_val_accuracy = 0.0\n",
        "best_model_wts = model.state_dict()\n",
        "\n",
        "# Training loop\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        logits = outputs.logits  # Extract logits\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        labels = torch.argmax(labels, dim=1) if labels.ndim > 1 else labels\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    train_accuracy = 100 * correct / total\n",
        "    train_losses.append(running_loss / len(trainloader))\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_running_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "            val_running_loss += loss.item()\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            labels = torch.argmax(labels, dim=1) if labels.ndim > 1 else labels\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_losses.append(val_running_loss / len(validloader))\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model_wts = model.state_dict()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]}, Train Accuracy: {train_accuracy}%, \"\n",
        "          f\"Val Loss: {val_losses[-1]}, Val Accuracy: {val_accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-QB5JBwkZqy",
        "outputId": "fba0f11c-cd79-4c19-d5f5-4a2562ea3b2b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 14.037919151823955, Train Accuracy: 66.74793350051083%, Val Loss: 13.861040437077902, Val Accuracy: 69.2282644089873%\n",
            "Epoch 2, Train Loss: 13.73492541591736, Train Accuracy: 72.39481749791028%, Val Loss: 13.79964398763266, Val Accuracy: 66.81862585477043%\n",
            "Epoch 3, Train Loss: 13.653787304035987, Train Accuracy: 74.21519457601931%, Val Loss: 13.797145150272721, Val Accuracy: 67.9040486269402%\n",
            "Epoch 4, Train Loss: 13.597147201754384, Train Accuracy: 74.37308442463082%, Val Loss: 13.857313451039264, Val Accuracy: 71.9743840225768%\n",
            "Epoch 5, Train Loss: 13.542849004883127, Train Accuracy: 75.11841738645862%, Val Loss: 13.919530768949823, Val Accuracy: 71.81157060675133%\n",
            "Epoch 6, Train Loss: 13.483187540290283, Train Accuracy: 75.42723135506641%, Val Loss: 13.753311869609787, Val Accuracy: 73.08151525018995%\n",
            "Epoch 7, Train Loss: 13.420162214855967, Train Accuracy: 75.64549085167641%, Val Loss: 13.834002261180954, Val Accuracy: 74.46000217084554%\n",
            "Epoch 8, Train Loss: 13.270042863498439, Train Accuracy: 78.81954119067521%, Val Loss: 13.71843197834061, Val Accuracy: 74.62281558667101%\n",
            "Epoch 9, Train Loss: 13.211332190077739, Train Accuracy: 79.12835515928299%, Val Loss: 13.742232192472281, Val Accuracy: 73.45055899272766%\n",
            "Epoch 10, Train Loss: 13.171231413215297, Train Accuracy: 79.30946410327854%, Val Loss: 13.769404342375607, Val Accuracy: 74.29718875502007%\n",
            "Epoch 11, Train Loss: 13.138227806877843, Train Accuracy: 79.52772359988855%, Val Loss: 13.776213956166464, Val Accuracy: 72.65820036904374%\n",
            "Epoch 12, Train Loss: 13.107441490868112, Train Accuracy: 79.4487786755828%, Val Loss: 13.815800911930191, Val Accuracy: 73.21176598285032%\n",
            "Epoch 13, Train Loss: 13.080465442945867, Train Accuracy: 79.46038822327482%, Val Loss: 13.84175110246283, Val Accuracy: 72.3868446760013%\n",
            "Epoch 14, Train Loss: 13.055945466064506, Train Accuracy: 79.65310671496239%, Val Loss: 13.862927674289688, Val Accuracy: 72.81015955714751%\n",
            "Epoch 15, Train Loss: 13.026997078734984, Train Accuracy: 80.2985975666388%, Val Loss: 13.898355418898495, Val Accuracy: 72.14805166612396%\n",
            "Epoch 16, Train Loss: 13.022349701714269, Train Accuracy: 80.52614470140243%, Val Loss: 13.903624622697333, Val Accuracy: 72.59307500271356%\n",
            "Epoch 17, Train Loss: 13.02059854674585, Train Accuracy: 80.33574811925327%, Val Loss: 13.918465882420062, Val Accuracy: 71.9852382502985%\n",
            "Epoch 18, Train Loss: 13.01702543796133, Train Accuracy: 80.36128912417573%, Val Loss: 13.918987484820875, Val Accuracy: 72.34342776511451%\n",
            "Epoch 19, Train Loss: 13.015385663796126, Train Accuracy: 80.24751555679391%, Val Loss: 13.94170899755026, Val Accuracy: 71.89840442852491%\n",
            "Epoch 20, Train Loss: 13.011922266884767, Train Accuracy: 80.31252902386923%, Val Loss: 13.93905281158815, Val Accuracy: 72.3542819928362%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Evaluate the Model on Test Split"
      ],
      "metadata": {
        "id": "KFgPAuM9CL7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        logits = outputs.logits\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        labels = torch.argmax(labels, dim=1) if labels.ndim > 1 else labels\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy with LoRA: {test_accuracy}%\")\n"
      ],
      "metadata": {
        "id": "PSbe_eUmCOoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7fa86e-f6c1-486b-f06d-13157bad4c6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy with LoRA: 71.70383199826803%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q884_X4QEErE"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}